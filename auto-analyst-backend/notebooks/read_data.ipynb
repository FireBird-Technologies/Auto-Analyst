{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCHEMA\n",
    "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey, DateTime, Text, Float, Boolean, JSON\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the base class for declarative models\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the Users table\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "    \n",
    "    user_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    username = Column(String, unique=True, nullable=False)\n",
    "    email = Column(String, unique=True, nullable=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    # Add relationship for cascade options\n",
    "    chats = relationship(\"Chat\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n",
    "    usage_records = relationship(\"ModelUsage\", back_populates=\"user\")\n",
    "\n",
    "# Define the Chats table\n",
    "class Chat(Base):\n",
    "    __tablename__ = 'chats'\n",
    "    \n",
    "    chat_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    user_id = Column(Integer, ForeignKey('users.user_id', ondelete=\"CASCADE\"), nullable=True)\n",
    "    title = Column(String, default='New Chat')\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    # Add relationships for cascade options\n",
    "    user = relationship(\"User\", back_populates=\"chats\")\n",
    "    messages = relationship(\"Message\", back_populates=\"chat\", cascade=\"all, delete-orphan\")\n",
    "    usage_records = relationship(\"ModelUsage\", back_populates=\"chat\")\n",
    "\n",
    "# Define the Messages table\n",
    "class Message(Base):\n",
    "    __tablename__ = 'messages'\n",
    "    \n",
    "    message_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    chat_id = Column(Integer, ForeignKey('chats.chat_id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    sender = Column(String, nullable=False)  # 'user' or 'ai'\n",
    "    content = Column(Text, nullable=False)\n",
    "    timestamp = Column(DateTime, default=datetime.utcnow)\n",
    "    # Add relationship for cascade options\n",
    "    chat = relationship(\"Chat\", back_populates=\"messages\")\n",
    "    feedback = relationship(\"MessageFeedback\", back_populates=\"message\", uselist=False, cascade=\"all, delete-orphan\")\n",
    "\n",
    "# Define the Model Usage table\n",
    "class ModelUsage(Base):\n",
    "    \"\"\"Tracks AI model usage metrics for analytics and billing purposes.\"\"\"\n",
    "    __tablename__ = 'model_usage'\n",
    "    \n",
    "    usage_id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey('users.user_id', ondelete=\"SET NULL\"), nullable=True)\n",
    "    chat_id = Column(Integer, ForeignKey('chats.chat_id', ondelete=\"SET NULL\"), nullable=True)\n",
    "    model_name = Column(String(100), nullable=False)\n",
    "    provider = Column(String(50), nullable=False)\n",
    "    prompt_tokens = Column(Integer, default=0)\n",
    "    completion_tokens = Column(Integer, default=0)\n",
    "    total_tokens = Column(Integer, default=0)\n",
    "    query_size = Column(Integer, default=0)  # Size in characters\n",
    "    response_size = Column(Integer, default=0)  # Size in characters\n",
    "    cost = Column(Float, default=0.0)  # Cost in USD\n",
    "    timestamp = Column(DateTime, default=datetime.utcnow)\n",
    "    is_streaming = Column(Boolean, default=False)\n",
    "    request_time_ms = Column(Integer, default=0)  # Request processing time in milliseconds\n",
    "    # Add relationships\n",
    "    user = relationship(\"User\", back_populates=\"usage_records\")\n",
    "    chat = relationship(\"Chat\", back_populates=\"usage_records\")\n",
    "\n",
    "# Define the Code Execution table\n",
    "class CodeExecution(Base):\n",
    "    \"\"\"Tracks code execution attempts and results for analysis and debugging.\"\"\"\n",
    "    __tablename__ = 'code_executions'\n",
    "    \n",
    "    execution_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    message_id = Column(Integer, ForeignKey('messages.message_id', ondelete=\"CASCADE\"), nullable=True)\n",
    "    chat_id = Column(Integer, ForeignKey('chats.chat_id', ondelete=\"CASCADE\"), nullable=True)\n",
    "    user_id = Column(Integer, ForeignKey('users.user_id', ondelete=\"SET NULL\"), nullable=True)\n",
    "    \n",
    "    # Code tracking\n",
    "    initial_code = Column(Text, nullable=True)  # First version of code submitted\n",
    "    latest_code = Column(Text, nullable=True)  # Most recent version of code\n",
    "    \n",
    "    # Execution results\n",
    "    is_successful = Column(Boolean, default=False)\n",
    "    output = Column(Text, nullable=True)  # Full output including errors\n",
    "    \n",
    "    # Model and agent information\n",
    "    model_provider = Column(String(50), nullable=True)\n",
    "    model_name = Column(String(100), nullable=True)\n",
    "    model_temperature = Column(Float, nullable=True)\n",
    "    model_max_tokens = Column(Integer, nullable=True)\n",
    "    \n",
    "    # Failure information\n",
    "    failed_agents = Column(Text, nullable=True)  # JSON list of agent names that failed\n",
    "    error_messages = Column(Text, nullable=True)  # JSON map of error messages by agent\n",
    "    \n",
    "    # Metadata\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "class MessageFeedback(Base):\n",
    "    \"\"\"Tracks user feedback and model settings for each message.\"\"\"\n",
    "    __tablename__ = 'message_feedback'\n",
    "    \n",
    "    feedback_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    message_id = Column(Integer, ForeignKey('messages.message_id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    \n",
    "    # User feedback\n",
    "    rating = Column(Integer, nullable=True)  # Star rating (1-5)\n",
    "    \n",
    "    # Model settings used for this message\n",
    "    model_name = Column(String(100), nullable=True)\n",
    "    model_provider = Column(String(50), nullable=True)\n",
    "    temperature = Column(Float, nullable=True)\n",
    "    max_tokens = Column(Integer, nullable=True)\n",
    "\n",
    "    # Metadata\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    # Relationship\n",
    "    message = relationship(\"Message\", back_populates=\"feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_10040\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id              username                             email  \\\n",
      "0          2         Ashad Qureshi               ashadq345@gmail.com   \n",
      "1         12         Ashad qureshi              ashad001sp@gmail.com   \n",
      "2         13         Arslan Shahid  arslan@firebird-technologies.com   \n",
      "3         15              gdkvd _3               gdkvd2048@gmail.com   \n",
      "4         55           Zain Sarwar         phicreative1997@gmail.com   \n",
      "..       ...                   ...                               ...   \n",
      "109      493  Дмитрий Колесниченко                 zeter11@gmail.com   \n",
      "110      494                 Ден Д               ppachutin@gmail.com   \n",
      "111      495       Askhat Urazbaev                  askhat@gmail.com   \n",
      "112      499   Kaviradj Krishnadas      kaviradjkrishnadas@gmail.com   \n",
      "113      500        Настя Куликова         nastya.kul.1703@gmail.com   \n",
      "\n",
      "                    created_at  \n",
      "0   2025-04-28 12:54:04.329562  \n",
      "1   2025-04-28 19:29:46.374201  \n",
      "2   2025-04-29 03:13:53.081492  \n",
      "3   2025-04-29 17:38:44.001372  \n",
      "4   2025-05-03 03:14:03.519069  \n",
      "..                         ...  \n",
      "109 2025-05-25 07:36:15.660211  \n",
      "110 2025-05-25 07:57:12.638008  \n",
      "111 2025-05-25 09:29:41.253568  \n",
      "112 2025-05-25 10:20:52.027678  \n",
      "113 2025-05-25 10:30:35.728665  \n",
      "\n",
      "[114 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_10040\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count\n",
      "0    114\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"auto-analyst-db.cyfmmwg0gt9b.us-east-1.rds.amazonaws.com\",\n",
    "    \"database\": \"autoanalystdb\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"L6sxfEJ2Dx87xNEGkA9y\"\n",
    "}\n",
    "\n",
    "def run_query(query):\n",
    "    \"\"\"\n",
    "    Executes a SQL query and returns the result as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with psycopg2.connect(**db_params) as conn:\n",
    "            return pd.read_sql_query(query, conn)\n",
    "    except Exception as e:\n",
    "        print(\"Error executing query:\", e)\n",
    "\n",
    "# Example usage\n",
    "query = \"SELECT * FROM USERS\"\n",
    "df = run_query(query)\n",
    "\n",
    "# non guest users where username doest not have guest in it\n",
    "query = \"SELECT * FROM USERS WHERE username NOT LIKE '%guest%'\"\n",
    "df = run_query(query)\n",
    "\n",
    "print(df)\n",
    "# count of non guest users\n",
    "query = \"SELECT COUNT(*) FROM USERS WHERE username NOT LIKE '%guest%'\"\n",
    "df = run_query(query)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_15384\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           username                       email\n",
      "0    guest_4bf1ca45  guest_4bf1ca45@example.com\n",
      "1    guest_23889f80  guest_23889f80@example.com\n",
      "2    guest_f467f92a  guest_f467f92a@example.com\n",
      "3    guest_0ca4ce97  guest_0ca4ce97@example.com\n",
      "4    guest_05a682e7  guest_05a682e7@example.com\n",
      "..              ...                         ...\n",
      "359  guest_4d76fe69  guest_4d76fe69@example.com\n",
      "360  guest_d0f07420  guest_d0f07420@example.com\n",
      "361  guest_da82a77b  guest_da82a77b@example.com\n",
      "362  guest_3b61e9e6  guest_3b61e9e6@example.com\n",
      "363  guest_759ce6fd  guest_759ce6fd@example.com\n",
      "\n",
      "[364 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT u.username, u.email\n",
    "FROM users u\n",
    "LEFT JOIN chats c ON u.user_id = c.user_id\n",
    "WHERE c.chat_id IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "df = run_query(query)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_15384\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count\n",
      "0    439\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM USERS\"\n",
    "print(run_query(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_15384\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    usage_id  user_id  chat_id  query_size  response_size  total_tokens\n",
      "0       1122      406    296.0           4             52            12\n",
      "1       1107      397    292.0           4             52            12\n",
      "2       1029      334    267.0           4             52            12\n",
      "3        985      332    261.0           4             52            12\n",
      "4        926      313    251.0           4             52            12\n",
      "5        924      297    242.0           4             52            12\n",
      "6        922      297    242.0           4             52            12\n",
      "7        917      311    250.0           4             52            12\n",
      "8        915      311    250.0           4             52            12\n",
      "9        913      311    250.0           4             52            12\n",
      "10       911      311    250.0           4             52            12\n",
      "11       909      311    250.0           4             52            12\n",
      "12       866      291    239.0           4             52            12\n",
      "13       864      291    239.0           4             52            12\n",
      "14       862      291    239.0           4             52            12\n",
      "15       860      289    238.0           4             52            12\n",
      "16       842      282    234.0           4             52            12\n",
      "17       832        1    229.0           4             52            12\n",
      "18       831        1    229.0           5            260            53\n",
      "19       830        1    229.0           4             52            12\n",
      "20       829        1    229.0           5            260            53\n",
      "21       820      271    225.0           4             52            12\n",
      "22       815      261    223.0           4             52            12\n",
      "23       542      111    149.0           4             52            12\n",
      "24       541      111    149.0           2            130            25\n",
      "25       532        1    148.0           4             52            12\n",
      "26       494      127    141.0           4             52            12\n",
      "27       490      127    141.0           4             52            12\n",
      "28       486      127    141.0           4             52            12\n",
      "29       484      127      NaN           4             52            12\n",
      "30       482      127      NaN           4             52            12\n",
      "31       480      127      NaN           4             52            12\n",
      "32       474      127      NaN           4             52            12\n",
      "33       472      127      NaN           4             52            12\n",
      "34       466      127      NaN           4             52            12\n",
      "35       465      127      NaN           4            133            24\n",
      "36       451        1    135.0           4             52            12\n",
      "37       437       12    104.0           4             52            12\n",
      "38       425        1    131.0           4             52            12\n",
      "39       420        1    131.0           4             52            12\n",
      "40       400        1    123.0           4             52            12\n",
      "41       398        1    123.0           4             52            12\n",
      "42       396        1    123.0           4             52            12\n",
      "43       394        1    123.0           4             52            12\n",
      "44       307       13    111.0           4             52            12\n",
      "45       195       15      NaN           4             52            12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_15384\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    message_id  chat_id sender  content_length                  timestamp\n",
      "0         1078      360     ai            7313 2025-05-25 08:02:20.836187\n",
      "1         1077      360   user              44 2025-05-25 08:01:40.221171\n",
      "2         1076      359     ai            3078 2025-05-25 07:57:34.598905\n",
      "3         1075      359   user              35 2025-05-25 07:57:31.715095\n",
      "4         1074      358     ai            3695 2025-05-25 07:22:11.706279\n",
      "..         ...      ...    ...             ...                        ...\n",
      "95         983      321     ai             141 2025-05-23 16:03:11.440771\n",
      "96         982      321   user              15 2025-05-23 16:03:04.359967\n",
      "97         981      320     ai             153 2025-05-23 15:58:33.625452\n",
      "98         980      320   user              15 2025-05-23 15:58:23.179757\n",
      "99         979      319     ai             119 2025-05-23 15:54:03.901142\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_15384\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chat_id  user_id                       title                 created_at\n",
      "0       360    494.0              Trend Analysis 2025-05-25 08:01:39.978195\n",
      "1       359    494.0  Data Visualization Request 2025-05-25 07:57:31.477531\n",
      "2       358    492.0              Trend Analysis 2025-05-25 07:17:34.246120\n",
      "3       357    492.0  Data Visualization Request 2025-05-25 07:13:42.278123\n",
      "4       356    491.0               Data Insights 2025-05-25 06:34:33.813821\n",
      "..      ...      ...                         ...                        ...\n",
      "95      261    332.0               Обсуждение БД 2025-05-21 08:26:15.829362\n",
      "96      260    300.0             Поиск Топонимов 2025-05-21 08:03:58.749727\n",
      "97      258    328.0        Possible Connections 2025-05-21 07:39:01.992868\n",
      "98      257    326.0            Анализ белорусов 2025-05-21 05:31:31.122865\n",
      "99      256    324.0          Статистика покупок 2025-05-21 04:46:11.190074\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "queries = {\n",
    "    \"model_usage_small_queries\": \"\"\"\n",
    "        SELECT \n",
    "            usage_id, user_id, chat_id,\n",
    "            query_size, response_size, total_tokens\n",
    "        FROM model_usage\n",
    "        WHERE query_size < 10\n",
    "        ORDER BY timestamp DESC;\n",
    "    \"\"\",\n",
    "\n",
    "    \"messages_basic\": \"\"\"\n",
    "        SELECT \n",
    "            message_id, chat_id, sender, LENGTH(content) AS content_length, timestamp\n",
    "        FROM messages\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 100;\n",
    "    \"\"\",\n",
    "\n",
    "    \"chats_basic\": \"\"\"\n",
    "        SELECT \n",
    "            chat_id, user_id, title, created_at\n",
    "        FROM chats\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT 100;\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "print(run_query(queries[\"model_usage_small_queries\"]))\n",
    "print(run_query(queries[\"messages_basic\"]))\n",
    "print(run_query(queries[\"chats_basic\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashad\\AppData\\Local\\Temp\\ipykernel_10040\\2926610153.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Date: 2025-05-25 11:34:02.000333\n",
      "User: 500\n",
      "Model: claude-3-5-sonnet-latest\n",
      "Initial Code: # planner_preprocessing_agent code start\n",
      "\n",
      "# Create a copy of the original dataframe\n",
      "processed_df = df.copy()\n",
      "\n",
      "# Create analysis dictionary to store results\n",
      "label_analysis = {\n",
      "    'input_text': {\n",
      "        'total_rows': len(processed_df),\n",
      "        'starts_with_question': 0,\n",
      "        'contains_question': 0,\n",
      "        'contains_answer': 0\n",
      "    },\n",
      "    'target_text': {\n",
      "        'total_rows': len(processed_df),\n",
      "        'starts_with_answer': 0,\n",
      "        'contains_question': 0,\n",
      "        'contains_answer': 0\n",
      "    }\n",
      "}\n",
      "\n",
      "# Analyze input_text\n",
      "label_analysis['input_text']['starts_with_question'] = processed_df['input_text'].str.startswith('вопрос:').sum()\n",
      "label_analysis['input_text']['contains_question'] = processed_df['input_text'].str.contains('вопрос:', regex=False).sum()\n",
      "label_analysis['input_text']['contains_answer'] = processed_df['input_text'].str.contains('ответ:', regex=False).sum()\n",
      "\n",
      "# Analyze target_text\n",
      "label_analysis['target_text']['starts_with_answer'] = processed_df['target_text'].str.startswith('ответ:').sum()\n",
      "label_analysis['target_text']['contains_question'] = processed_df['target_text'].str.contains('вопрос:', regex=False).sum()\n",
      "label_analysis['target_text']['contains_answer'] = processed_df['target_text'].str.contains('ответ:', regex=False).sum()\n",
      "\n",
      "# Calculate percentages\n",
      "for column in ['input_text', 'target_text']:\n",
      "    total = label_analysis[column]['total_rows']\n",
      "    for key in label_analysis[column].keys():\n",
      "        if key != 'total_rows':\n",
      "            count = label_analysis[column][key]\n",
      "            percentage = (count / total) * 100\n",
      "            label_analysis[column][f'{key}_percentage'] = round(percentage, 2)\n",
      "\n",
      "# Find examples of inconsistent labeling\n",
      "inconsistent_questions = processed_df[~processed_df['input_text'].str.startswith('вопрос:')]['input_text'].head()\n",
      "inconsistent_answers = processed_df[processed_df['target_text'].str.startswith('ответ:')]['target_text'].head()\n",
      "\n",
      "# Add examples to analysis\n",
      "label_analysis['examples'] = {\n",
      "    'missing_question_label': inconsistent_questions.tolist(),\n",
      "    'unexpected_answer_label': inconsistent_answers.tolist()\n",
      "}\n",
      "\n",
      "# planner_preprocessing_agent code end\n",
      "Latest Code: # Create a copy of the original dataframe\n",
      "processed_df = df.copy()\n",
      "\n",
      "# Create analysis dictionary to store results\n",
      "label_analysis = {\n",
      "    'input_text': {\n",
      "        'total_rows': len(processed_df),\n",
      "        'starts_with_question': 0,\n",
      "        'contains_question': 0,\n",
      "        'contains_answer': 0\n",
      "    },\n",
      "    'target_text': {\n",
      "        'total_rows': len(processed_df),\n",
      "        'starts_with_answer': 0,\n",
      "        'contains_question': 0,\n",
      "        'contains_answer': 0\n",
      "    }\n",
      "}\n",
      "\n",
      "# Analyze input_text\n",
      "label_analysis['input_text']['starts_with_question'] = processed_df['input_text'].str.lower().str.startswith('вопрос:').sum()\n",
      "label_analysis['input_text']['contains_question'] = processed_df['input_text'].str.lower().str.contains('вопрос:', regex=False).sum()\n",
      "label_analysis['input_text']['contains_answer'] = processed_df['input_text'].str.lower().str.contains('ответ:', regex=False).sum()\n",
      "\n",
      "# Analyze target_text\n",
      "label_analysis['target_text']['starts_with_answer'] = processed_df['target_text'].str.lower().str.startswith('ответ:').sum()\n",
      "label_analysis['target_text']['contains_question'] = processed_df['target_text'].str.lower().str.contains('вопрос:', regex=False).sum()\n",
      "label_analysis['target_text']['contains_answer'] = processed_df['target_text'].str.lower().str.contains('ответ:', regex=False).sum()\n",
      "\n",
      "# Calculate percentages - сначала собираем ключи, потом итерируем\n",
      "for column in ['input_text', 'target_text']:\n",
      "    total = label_analysis[column]['total_rows']\n",
      "    # Создаем список ключей до итерации\n",
      "    keys = list(label_analysis[column].keys())\n",
      "    for key in keys:\n",
      "        if key != 'total_rows':\n",
      "            count = label_analysis[column][key]\n",
      "            percentage = (count / total) * 100\n",
      "            label_analysis[column][f'{key}_percentage'] = round(percentage, 2)\n",
      "\n",
      "# Find examples of inconsistent labeling\n",
      "inconsistent_questions = processed_df[~processed_df['input_text'].str.lower().str.startswith('вопрос:')]['input_text'].head()\n",
      "inconsistent_answers = processed_df[processed_df['target_text'].str.lower().str.startswith('ответ:')]['target_text'].head()\n",
      "\n",
      "# Add examples to analysis\n",
      "label_analysis['examples'] = {\n",
      "    'missing_question_label': inconsistent_questions.tolist(),\n",
      "    'unexpected_answer_label': inconsistent_answers.tolist()\n",
      "}\n",
      "Output: \n",
      "Status: Success\n",
      "Failed Agents: [\"planner_preprocessing_agent\"]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Date: 2025-05-25 11:00:25.521351\n",
      "User: 500\n",
      "Model: claude-3-5-sonnet-latest\n",
      "Initial Code: # planner_preprocessing_agent code start\n",
      "\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "# Create a copy of the original dataframe\n",
      "processed_df = df.copy()\n",
      "\n",
      "# Function to clean text while preserving Russian characters\n",
      "def clean_russian_text(text):\n",
      "    # Remove 'вопрос: ' prefix if present\n",
      "    text = re.sub(r'^вопрос:\\s*', '', text)\n",
      "    \n",
      "    # Keep only Russian letters and spaces\n",
      "    text = re.sub(r'[^а-яА-ЯёЁ\\s]', ' ', text)\n",
      "    \n",
      "    # Normalize whitespace\n",
      "    text = ' '.join(text.split())\n",
      "    \n",
      "    # Convert to lowercase\n",
      "    text = text.lower()\n",
      "    \n",
      "    return text\n",
      "\n",
      "# Create cleaned questions collection\n",
      "cleaned_questions = processed_df['input_text'].apply(clean_russian_text)\n",
      "\n",
      "# Create cleaned answers collection\n",
      "cleaned_answers = processed_df['target_text'].apply(clean_russian_text)\n",
      "\n",
      "# planner_preprocessing_agent code end\n",
      "\n",
      "\n",
      "\n",
      "# planner_data_viz_agent code start\n",
      "\n",
      "import plotly.graph_objects as go\n",
      "from plotly.subplots import make_subplots\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "\n",
      "# Create word frequency counts\n",
      "question_words = ' '.join(cleaned_questions).split()\n",
      "answer_words = ' '.join(cleaned_answers).split()\n",
      "\n",
      "question_freq = Counter(question_words)\n",
      "answer_freq = Counter(answer_words)\n",
      "\n",
      "# Create scatter plots to simulate word clouds\n",
      "fig = make_subplots(rows=1, cols=2, \n",
      "                    subplot_titles=('Облако слов из вопросов', 'Облако слов из ответов'),\n",
      "                    horizontal_spacing=0.1)\n",
      "\n",
      "# Questions word cloud\n",
      "words_q = list(question_freq.keys())\n",
      "freqs_q = list(question_freq.values())\n",
      "sizes_q = np.array(freqs_q) / max(freqs_q) * 50  # Normalize sizes\n",
      "\n",
      "fig.add_trace(\n",
      "    go.Scatter(\n",
      "        x=np.random.randn(len(words_q)),\n",
      "        y=np.random.randn(len(words_q)),\n",
      "        text=words_q,\n",
      "        mode='text',\n",
      "        textfont=dict(\n",
      "            size=sizes_q,\n",
      "            color='royalblue'\n",
      "        ),\n",
      "        hoverinfo='text+text',\n",
      "        name='Вопросы'\n",
      "    ),\n",
      "    row=1, col=1\n",
      ")\n",
      "\n",
      "# Answers word cloud\n",
      "words_a = list(answer_freq.keys())\n",
      "freqs_a = list(answer_freq.values())\n",
      "sizes_a = np.array(freqs_a) / max(freqs_a) * 50  # Normalize sizes\n",
      "\n",
      "fig.add_trace(\n",
      "    go.Scatter(\n",
      "        x=np.random.randn(len(words_a)),\n",
      "        y=np.random.randn(len(words_a)),\n",
      "        text=words_a,\n",
      "        mode='text',\n",
      "        textfont=dict(\n",
      "            size=sizes_a,\n",
      "            color='firebrick'\n",
      "        ),\n",
      "        hoverinfo='text+text',\n",
      "        name='Ответы'\n",
      "    ),\n",
      "    row=1, col=2\n",
      ")\n",
      "\n",
      "# Update layout\n",
      "fig.update_layout(\n",
      "    template='plotly_white',\n",
      "    showlegend=False,\n",
      "    height=1200,\n",
      "    width=1000,\n",
      "    title={\n",
      "        'text': 'Сравнение частотности слов в вопросах и ответах',\n",
      "        'y': 0.95,\n",
      "        'x': 0.5,\n",
      "        'xanchor': 'center',\n",
      "        'yanchor': 'top'\n",
      "    }\n",
      ")\n",
      "\n",
      "# Update axes\n",
      "for i in [1, 2]:\n",
      "    fig.update_xaxes(showgrid=False, showticklabels=False, zeroline=False, row=1, col=i)\n",
      "    fig.update_yaxes(showgrid=False, showticklabels=False, zeroline=False, row=1, col=i)\n",
      "\n",
      "fig.show()\n",
      "\n",
      "# planner_data_viz_agent code end\n",
      "Latest Code: # planner_preprocessing_agent code start\n",
      "\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "# Create a copy of the original dataframe\n",
      "processed_df = df.copy()\n",
      "\n",
      "# Function to clean text while preserving Russian characters\n",
      "def clean_russian_text(text):\n",
      "    # Remove 'вопрос: ' prefix if present\n",
      "    text = re.sub(r'^вопрос:\\s*', '', text)\n",
      "    \n",
      "    # Keep only Russian letters and spaces\n",
      "    text = re.sub(r'[^а-яА-ЯёЁ\\s]', ' ', text)\n",
      "    \n",
      "    # Normalize whitespace\n",
      "    text = ' '.join(text.split())\n",
      "    \n",
      "    # Convert to lowercase\n",
      "    text = text.lower()\n",
      "    \n",
      "    return text\n",
      "\n",
      "# Create cleaned questions collection\n",
      "cleaned_questions = processed_df['input_text'].apply(clean_russian_text)\n",
      "\n",
      "# Create cleaned answers collection\n",
      "cleaned_answers = processed_df['target_text'].apply(clean_russian_text)\n",
      "\n",
      "# planner_preprocessing_agent code end\n",
      "\n",
      "\n",
      "\n",
      "# planner_data_viz_agent code start\n",
      "\n",
      "import plotly.graph_objects as go\n",
      "from plotly.subplots import make_subplots\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "\n",
      "# Create word frequency counts\n",
      "question_words = ' '.join(cleaned_questions).split()\n",
      "answer_words = ' '.join(cleaned_answers).split()\n",
      "\n",
      "question_freq = Counter(question_words)\n",
      "answer_freq = Counter(answer_words)\n",
      "\n",
      "# Create scatter plots to simulate word clouds\n",
      "fig = make_subplots(rows=1, cols=2, \n",
      "                    subplot_titles=('Облако слов из вопросов', 'Облако слов из ответов'),\n",
      "                    horizontal_spacing=0.1)\n",
      "\n",
      "# Questions word cloud\n",
      "words_q = list(question_freq.keys())\n",
      "freqs_q = list(question_freq.values())\n",
      "sizes_q = np.array(freqs_q) / max(freqs_q) * 50  # Normalize sizes\n",
      "\n",
      "fig.add_trace(\n",
      "    go.Scatter(\n",
      "        x=np.random.randn(len(words_q)),\n",
      "        y=np.random.randn(len(words_q)),\n",
      "        text=words_q,\n",
      "        mode='text',\n",
      "        textfont=dict(\n",
      "            size=sizes_q,\n",
      "            color='royalblue'\n",
      "        ),\n",
      "        hoverinfo='text+text',\n",
      "        name='Вопросы'\n",
      "    ),\n",
      "    row=1, col=1\n",
      ")\n",
      "\n",
      "# Answers word cloud\n",
      "words_a = list(answer_freq.keys())\n",
      "freqs_a = list(answer_freq.values())\n",
      "sizes_a = np.array(freqs_a) / max(freqs_a) * 50  # Normalize sizes\n",
      "\n",
      "fig.add_trace(\n",
      "    go.Scatter(\n",
      "        x=np.random.randn(len(words_a)),\n",
      "        y=np.random.randn(len(words_a)),\n",
      "        text=words_a,\n",
      "        mode='text',\n",
      "        textfont=dict(\n",
      "            size=sizes_a,\n",
      "            color='firebrick'\n",
      "        ),\n",
      "        hoverinfo='text+text',\n",
      "        name='Ответы'\n",
      "    ),\n",
      "    row=1, col=2\n",
      ")\n",
      "\n",
      "# Update layout\n",
      "fig.update_layout(\n",
      "    template='plotly_white',\n",
      "    showlegend=False,\n",
      "    height=1200,\n",
      "    width=1000,\n",
      "    title={\n",
      "        'text': 'Сравнение частотности слов в вопросах и ответах',\n",
      "        'y': 0.95,\n",
      "        'x': 0.5,\n",
      "        'xanchor': 'center',\n",
      "        'yanchor': 'top'\n",
      "    }\n",
      ")\n",
      "\n",
      "# Update axes\n",
      "for i in [1, 2]:\n",
      "    fig.update_xaxes(showgrid=False, showticklabels=False, zeroline=False, row=1, col=i)\n",
      "    fig.update_yaxes(showgrid=False, showticklabels=False, zeroline=False, row=1, col=i)\n",
      "\n",
      "fig.show()\n",
      "\n",
      "# planner_data_viz_agent code end\n",
      "Output: \n",
      "\n",
      "=== ERROR IN PLANNER_DATA_VIZ_AGENT ===\n",
      "Error in planner_data_viz_agent: \n",
      "    Invalid element(s) received for the 'size' property of scatter.textfont\n",
      "        Invalid elements include: [0.3597122302158274, 0.5995203836930456, 0.8393285371702638, 0.1199040767386091, 0.3597122302158274, 0.3597122302158274, 0.1199040767386091, 0.7194244604316548, 0.4796163069544364, 0.4796163069544364]\n",
      "\n",
      "    The 'size' property is a number and may be specified as:\n",
      "      - An int or float in the interval [1, inf]\n",
      "      - A tuple, list, or one-dimensional numpy array of the above\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/format_response.py\", line 318, in execute_code_from_markdown\n",
      "    exec(block_code, context)  # Execute the block\n",
      "\n",
      "Problem at this location:\n",
      "23: \n",
      "24: fig.add_trace(\n",
      "25: >>>     go.Scatter( <<<\n",
      "26:         x=np.random.randn(len(words_q)),\n",
      "27:         y=np.random.randn(len(words_q)),\n",
      "\n",
      "Full error details:\n",
      "    The 'size' property is a number and may be specified as:\n",
      "      - An int or float in the interval [1, inf]\n",
      "      - A tuple, list, or one-dimensional numpy array of the above\n",
      "\n",
      "Status: Failed\n",
      "Failed Agents: [\"planner_data_viz_agent\"]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Date: 2025-05-25 10:50:21.971316\n",
      "User: 500\n",
      "Model: claude-3-5-sonnet-latest\n",
      "Initial Code: # planner_statistical_analytics_agent code start\n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import describe\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "\n",
      "def calculate_text_statistics(df):\n",
      "    # Initialize statistics dictionary\n",
      "    text_stats = {}\n",
      "    \n",
      "    # Length statistics\n",
      "    text_stats['input_lengths'] = df['input_text'].str.len().describe().to_dict()\n",
      "    text_stats['target_lengths'] = df['target_text'].str.len().describe().to_dict()\n",
      "    \n",
      "    # Vocabulary statistics\n",
      "    input_words = set(' '.join(df['input_text']).split())\n",
      "    target_words = set(' '.join(df['target_text']).split())\n",
      "    text_stats['input_vocab_size'] = len(input_words)\n",
      "    text_stats['target_vocab_size'] = len(target_words)\n",
      "    text_stats['total_unique_words'] = len(input_words.union(target_words))\n",
      "    \n",
      "    # Calculate diversity metrics\n",
      "    tfidf = TfidfVectorizer()\n",
      "    input_tfidf = tfidf.fit_transform(df['input_text'])\n",
      "    target_tfidf = tfidf.fit_transform(df['target_text'])\n",
      "    \n",
      "    diversity_metrics = {\n",
      "        'input_similarity': cosine_similarity(input_tfidf).mean(),\n",
      "        'target_similarity': cosine_similarity(target_tfidf).mean(),\n",
      "        'vocab_overlap': len(input_words.intersection(target_words)) / len(input_words.union(target_words))\n",
      "    }\n",
      "    \n",
      "    return text_stats, diversity_metrics\n",
      "\n",
      "# Calculate statistics\n",
      "text_stats, diversity_metrics = calculate_text_statistics(df)\n",
      "\n",
      "# Print summary statistics\n",
      "print(\"\\nText Statistics:\")\n",
      "print(f\"Input text length stats: {text_stats['input_lengths']}\")\n",
      "print(f\"Target text length stats: {text_stats['target_lengths']}\")\n",
      "print(f\"Input vocabulary size: {text_stats['input_vocab_size']}\")\n",
      "print(f\"Target vocabulary size: {text_stats['target_vocab_size']}\")\n",
      "print(f\"Total unique words: {text_stats['total_unique_words']}\")\n",
      "\n",
      "print(\"\\nDiversity Metrics:\")\n",
      "print(f\"Average input text similarity: {diversity_metrics['input_similarity']:.3f}\")\n",
      "print(f\"Average target text similarity: {diversity_metrics['target_similarity']:.3f}\")\n",
      "print(f\"Vocabulary overlap ratio: {diversity_metrics['vocab_overlap']:.3f}\")\n",
      "\n",
      "# planner_statistical_analytics_agent code end\n",
      "\n",
      "\n",
      "\n",
      "# planner_data_viz_agent code start\n",
      "\n",
      "import plotly.graph_objects as go\n",
      "from plotly.subplots import make_subplots\n",
      "\n",
      "# Create subplots for distributions\n",
      "fig = make_subplots(\n",
      "    rows=2, cols=2,\n",
      "    subplot_titles=('Input Text Length Distribution', 'Target Text Length Distribution',\n",
      "                   'Token Frequency Distribution', 'Text Similarity Heatmap')\n",
      ")\n",
      "\n",
      "# Input text length distribution\n",
      "fig.add_trace(\n",
      "    go.Histogram(\n",
      "        x=text_stats['input_lengths'],\n",
      "        name='Input Length',\n",
      "        nbinsx=30,\n",
      "        marker_color='#1f77b4'\n",
      "    ),\n",
      "    row=1, col=1\n",
      ")\n",
      "\n",
      "# Target text length distribution\n",
      "fig.add_trace(\n",
      "    go.Histogram(\n",
      "        x=text_stats['target_lengths'],\n",
      "        name='Target Length',\n",
      "        nbinsx=30,\n",
      "        marker_color='#ff7f0e'\n",
      "    ),\n",
      "    row=1, col=2\n",
      ")\n",
      "\n",
      "# Token frequency distribution\n",
      "fig.add_trace(\n",
      "    go.Bar(\n",
      "        x=list(range(len(text_stats['token_frequencies']))),\n",
      "        y=text_stats['token_frequencies'],\n",
      "        name='Token Frequency',\n",
      "        marker_color='#2ca02c'\n",
      "    ),\n",
      "    row=2, col=1\n",
      ")\n",
      "\n",
      "# Text similarity heatmap\n",
      "fig.add_trace(\n",
      "    go.Heatmap(\n",
      "        z=diversity_metrics['similarity_matrix'],\n",
      "        colorscale='RdBu',\n",
      "        name='Similarity'\n",
      "    ),\n",
      "    row=2, col=2\n",
      ")\n",
      "\n",
      "# Update layout\n",
      "fig.update_layout(\n",
      "    height=1200,\n",
      "    width=1000,\n",
      "    title='Dataset Analysis for GPT Fine-tuning',\n",
      "    template='plotly_white',\n",
      "    showlegend=True\n",
      ")\n",
      "\n",
      "# Update axes\n",
      "fig.update_xaxes(title_text='<b>Text Length (characters)</b>', row=1, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_xaxes(title_text='<b>Text Length (characters)</b>', row=1, col=2, gridwidth=1, linewidth=0.2)\n",
      "fig.update_xaxes(title_text='<b>Token Rank</b>', row=2, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_xaxes(title_text='<b>Sample Index</b>', row=2, col=2, gridwidth=1, linewidth=0.2)\n",
      "\n",
      "fig.update_yaxes(title_text='<b>Frequency</b>', row=1, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_yaxes(title_text='<b>Frequency</b>', row=1, col=2, gridwidth=1, linewidth=0.2)\n",
      "fig.update_yaxes(title_text='<b>Frequency</b>', row=2, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_yaxes(title_text='<b>Sample Index</b>', row=2, col=2, gridwidth=1, linewidth=0.2)\n",
      "\n",
      "fig.show()\n",
      "\n",
      "# planner_data_viz_agent code end\n",
      "Latest Code: # planner_statistical_analytics_agent code start\n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import describe\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "\n",
      "def calculate_text_statistics(df):\n",
      "    # Initialize statistics dictionary\n",
      "    text_stats = {}\n",
      "    \n",
      "    # Length statistics\n",
      "    text_stats['input_lengths'] = df['input_text'].str.len().describe().to_dict()\n",
      "    text_stats['target_lengths'] = df['target_text'].str.len().describe().to_dict()\n",
      "    \n",
      "    # Vocabulary statistics\n",
      "    input_words = set(' '.join(df['input_text']).split())\n",
      "    target_words = set(' '.join(df['target_text']).split())\n",
      "    text_stats['input_vocab_size'] = len(input_words)\n",
      "    text_stats['target_vocab_size'] = len(target_words)\n",
      "    text_stats['total_unique_words'] = len(input_words.union(target_words))\n",
      "    \n",
      "    # Calculate diversity metrics\n",
      "    tfidf = TfidfVectorizer()\n",
      "    input_tfidf = tfidf.fit_transform(df['input_text'])\n",
      "    target_tfidf = tfidf.fit_transform(df['target_text'])\n",
      "    \n",
      "    diversity_metrics = {\n",
      "        'input_similarity': cosine_similarity(input_tfidf).mean(),\n",
      "        'target_similarity': cosine_similarity(target_tfidf).mean(),\n",
      "        'vocab_overlap': len(input_words.intersection(target_words)) / len(input_words.union(target_words))\n",
      "    }\n",
      "    \n",
      "    return text_stats, diversity_metrics\n",
      "\n",
      "# Calculate statistics\n",
      "text_stats, diversity_metrics = calculate_text_statistics(df)\n",
      "\n",
      "# Print summary statistics\n",
      "print(\"\\nText Statistics:\")\n",
      "print(f\"Input text length stats: {text_stats['input_lengths']}\")\n",
      "print(f\"Target text length stats: {text_stats['target_lengths']}\")\n",
      "print(f\"Input vocabulary size: {text_stats['input_vocab_size']}\")\n",
      "print(f\"Target vocabulary size: {text_stats['target_vocab_size']}\")\n",
      "print(f\"Total unique words: {text_stats['total_unique_words']}\")\n",
      "\n",
      "print(\"\\nDiversity Metrics:\")\n",
      "print(f\"Average input text similarity: {diversity_metrics['input_similarity']:.3f}\")\n",
      "print(f\"Average target text similarity: {diversity_metrics['target_similarity']:.3f}\")\n",
      "print(f\"Vocabulary overlap ratio: {diversity_metrics['vocab_overlap']:.3f}\")\n",
      "\n",
      "# planner_statistical_analytics_agent code end\n",
      "\n",
      "\n",
      "\n",
      "# planner_data_viz_agent code start\n",
      "\n",
      "import plotly.graph_objects as go\n",
      "from plotly.subplots import make_subplots\n",
      "\n",
      "# Create subplots for distributions\n",
      "fig = make_subplots(\n",
      "    rows=2, cols=2,\n",
      "    subplot_titles=('Input Text Length Distribution', 'Target Text Length Distribution',\n",
      "                   'Token Frequency Distribution', 'Text Similarity Heatmap')\n",
      ")\n",
      "\n",
      "# Input text length distribution\n",
      "fig.add_trace(\n",
      "    go.Histogram(\n",
      "        x=text_stats['input_lengths'],\n",
      "        name='Input Length',\n",
      "        nbinsx=30,\n",
      "        marker_color='#1f77b4'\n",
      "    ),\n",
      "    row=1, col=1\n",
      ")\n",
      "\n",
      "# Target text length distribution\n",
      "fig.add_trace(\n",
      "    go.Histogram(\n",
      "        x=text_stats['target_lengths'],\n",
      "        name='Target Length',\n",
      "        nbinsx=30,\n",
      "        marker_color='#ff7f0e'\n",
      "    ),\n",
      "    row=1, col=2\n",
      ")\n",
      "\n",
      "# Token frequency distribution\n",
      "fig.add_trace(\n",
      "    go.Bar(\n",
      "        x=list(range(len(text_stats['token_frequencies']))),\n",
      "        y=text_stats['token_frequencies'],\n",
      "        name='Token Frequency',\n",
      "        marker_color='#2ca02c'\n",
      "    ),\n",
      "    row=2, col=1\n",
      ")\n",
      "\n",
      "# Text similarity heatmap\n",
      "fig.add_trace(\n",
      "    go.Heatmap(\n",
      "        z=diversity_metrics['similarity_matrix'],\n",
      "        colorscale='RdBu',\n",
      "        name='Similarity'\n",
      "    ),\n",
      "    row=2, col=2\n",
      ")\n",
      "\n",
      "# Update layout\n",
      "fig.update_layout(\n",
      "    height=1200,\n",
      "    width=1000,\n",
      "    title='Dataset Analysis for GPT Fine-tuning',\n",
      "    template='plotly_white',\n",
      "    showlegend=True\n",
      ")\n",
      "\n",
      "# Update axes\n",
      "fig.update_xaxes(title_text='<b>Text Length (characters)</b>', row=1, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_xaxes(title_text='<b>Text Length (characters)</b>', row=1, col=2, gridwidth=1, linewidth=0.2)\n",
      "fig.update_xaxes(title_text='<b>Token Rank</b>', row=2, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_xaxes(title_text='<b>Sample Index</b>', row=2, col=2, gridwidth=1, linewidth=0.2)\n",
      "\n",
      "fig.update_yaxes(title_text='<b>Frequency</b>', row=1, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_yaxes(title_text='<b>Frequency</b>', row=1, col=2, gridwidth=1, linewidth=0.2)\n",
      "fig.update_yaxes(title_text='<b>Frequency</b>', row=2, col=1, gridwidth=1, linewidth=0.2)\n",
      "fig.update_yaxes(title_text='<b>Sample Index</b>', row=2, col=2, gridwidth=1, linewidth=0.2)\n",
      "\n",
      "fig.show()\n",
      "\n",
      "# planner_data_viz_agent code end\n",
      "Output: \n",
      "\n",
      "=== OUTPUT FROM PLANNER_STATISTICAL_ANALYTICS_AGENT ===\n",
      "\n",
      "Text Statistics:\n",
      "Input text length stats: {'count': 1014.0, 'mean': 58.45266272189349, 'std': 12.1963707243449, 'min': 24.0, '25%': 50.0, '50%': 58.0, '75%': 66.0, 'max': 101.0}\n",
      "Target text length stats: {'count': 1014.0, 'mean': 200.95463510848126, 'std': 68.48157038174318, 'min': 15.0, '25%': 152.0, '50%': 203.0, '75%': 249.0, 'max': 434.0}\n",
      "Input vocabulary size: 1737\n",
      "Target vocabulary size: 1789\n",
      "Total unique words: 3117\n",
      "\n",
      "Diversity Metrics:\n",
      "Average input text similarity: 0.028\n",
      "Average target text similarity: 0.035\n",
      "Vocabulary overlap ratio: 0.131\n",
      "\n",
      "\n",
      "\n",
      "=== ERROR IN PLANNER_DATA_VIZ_AGENT ===\n",
      "Error in planner_data_viz_agent: \n",
      "    Invalid value of type 'builtins.dict' received for the 'x' property of histogram\n",
      "        Received value: {'count': 1014.0, 'mean': 58.45266272189349, 'std': 12.1963707243449, 'min': 24.0, '25%': 50.0, '50%': 58.0, '75%': 66.0, 'max': 101.0}\n",
      "\n",
      "    The 'x' property is an array that may be specified as a tuple,\n",
      "    list, numpy array, or pandas Series\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/format_response.py\", line 318, in execute_code_from_markdown\n",
      "    exec(block_code, context)  # Execute the block\n",
      "\n",
      "Problem at this location:\n",
      "12: # Input text length distribution\n",
      "13: fig.add_trace(\n",
      "14: >>>     go.Histogram( <<<\n",
      "15:         x=text_stats['input_lengths'],\n",
      "16:         name='Input Length',\n",
      "\n",
      "Full error details:\n",
      "\n",
      "    The 'x' property is an array that may be specified as a tuple,\n",
      "    list, numpy array, or pandas Series\n",
      "\n",
      "Status: Failed\n",
      "Failed Agents: [\"planner_data_viz_agent\"]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Date: 2025-05-25 10:38:53.181413\n",
      "User: 500\n",
      "Model: claude-3-5-sonnet-latest\n",
      "Initial Code: # planner_preprocessing_agent code start\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "import re\n",
      "\n",
      "# Create a copy of the original dataframe\n",
      "processed_df = df.copy()\n",
      "\n",
      "# Function to clean text\n",
      "def clean_text(text):\n",
      "    # Remove special characters but keep Russian letters\n",
      "    text = re.sub(r'[^\\w\\s\\-\\?]', '', text)\n",
      "    return text.lower().strip()\n",
      "\n",
      "# Text analysis features\n",
      "processed_df['cleaned_input'] = processed_df['input_text'].apply(clean_text)\n",
      "processed_df['cleaned_target'] = processed_df['target_text'].apply(clean_text)\n",
      "\n",
      "# Calculate text lengths\n",
      "processed_df['question_length'] = processed_df['cleaned_input'].str.len()\n",
      "processed_df['answer_length'] = processed_df['cleaned_target'].str.len()\n",
      "\n",
      "# Create text_analysis dictionary\n",
      "text_analysis = {\n",
      "    'avg_question_length': processed_df['question_length'].mean(),\n",
      "    'avg_answer_length': processed_df['answer_length'].mean(),\n",
      "    'question_length_std': processed_df['question_length'].std(),\n",
      "    'answer_length_std': processed_df['answer_length'].std()\n",
      "}\n",
      "\n",
      "# Function to get word frequency\n",
      "def get_word_frequency(texts):\n",
      "    words = ' '.join(texts).split()\n",
      "    return Counter(words)\n",
      "\n",
      "# Calculate word frequencies\n",
      "question_freq = get_word_frequency(processed_df['cleaned_input'])\n",
      "answer_freq = get_word_frequency(processed_df['cleaned_target'])\n",
      "\n",
      "# Create pattern_analysis dictionary\n",
      "pattern_analysis = {\n",
      "    'common_question_words': dict(question_freq.most_common(10)),\n",
      "    'common_answer_words': dict(answer_freq.most_common(10)),\n",
      "    'questions_starting_with_why': sum(1 for q in processed_df['cleaned_input'] if 'почему' in q.lower()),\n",
      "    'questions_starting_with_what': sum(1 for q in processed_df['cleaned_input'] if 'что' in q.lower()),\n",
      "    'answers_with_steps': sum(1 for a in processed_df['cleaned_target'] if any(str(i) for i in range(1, 10) if str(i) in a))\n",
      "}\n",
      "\n",
      "# Add analysis results back to the dataframe\n",
      "processed_df['has_numbered_steps'] = processed_df['target_text'].apply(lambda x: any(str(i) for i in range(1, 10) if str(i) in x))\n",
      "processed_df['question_type'] = processed_df['cleaned_input'].apply(lambda x: 'why' if 'почему' in x else ('what' if 'что' in x else 'other'))\n",
      "\n",
      "# planner_preprocessing_agent code end\n",
      "Latest Code: # planner_preprocessing_agent code start\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "import re\n",
      "\n",
      "# Create a copy of the original dataframe\n",
      "processed_df = df.copy()\n",
      "\n",
      "# Function to clean text\n",
      "def clean_text(text):\n",
      "    # Remove special characters but keep Russian letters\n",
      "    text = re.sub(r'[^\\w\\s\\-\\?]', '', text)\n",
      "    return text.lower().strip()\n",
      "\n",
      "# Text analysis features\n",
      "processed_df['cleaned_input'] = processed_df['input_text'].apply(clean_text)\n",
      "processed_df['cleaned_target'] = processed_df['target_text'].apply(clean_text)\n",
      "\n",
      "# Calculate text lengths\n",
      "processed_df['question_length'] = processed_df['cleaned_input'].str.len()\n",
      "processed_df['answer_length'] = processed_df['cleaned_target'].str.len()\n",
      "\n",
      "# Create text_analysis dictionary\n",
      "text_analysis = {\n",
      "    'avg_question_length': processed_df['question_length'].mean(),\n",
      "    'avg_answer_length': processed_df['answer_length'].mean(),\n",
      "    'question_length_std': processed_df['question_length'].std(),\n",
      "    'answer_length_std': processed_df['answer_length'].std()\n",
      "}\n",
      "\n",
      "# Function to get word frequency\n",
      "def get_word_frequency(texts):\n",
      "    words = ' '.join(texts).split()\n",
      "    return Counter(words)\n",
      "\n",
      "# Calculate word frequencies\n",
      "question_freq = get_word_frequency(processed_df['cleaned_input'])\n",
      "answer_freq = get_word_frequency(processed_df['cleaned_target'])\n",
      "\n",
      "# Create pattern_analysis dictionary\n",
      "pattern_analysis = {\n",
      "    'common_question_words': dict(question_freq.most_common(10)),\n",
      "    'common_answer_words': dict(answer_freq.most_common(10)),\n",
      "    'questions_starting_with_why': sum(1 for q in processed_df['cleaned_input'] if 'почему' in q.lower()),\n",
      "    'questions_starting_with_what': sum(1 for q in processed_df['cleaned_input'] if 'что' in q.lower()),\n",
      "    'answers_with_steps': sum(1 for a in processed_df['cleaned_target'] if any(str(i) for i in range(1, 10) if str(i) in a))\n",
      "}\n",
      "\n",
      "# Add analysis results back to the dataframe\n",
      "processed_df['has_numbered_steps'] = processed_df['target_text'].apply(lambda x: any(str(i) for i in range(1, 10) if str(i) in x))\n",
      "processed_df['question_type'] = processed_df['cleaned_input'].apply(lambda x: 'why' if 'почему' in x else ('what' if 'что' in x else 'other'))\n",
      "\n",
      "# planner_preprocessing_agent code end\n",
      "Output: \n",
      "Status: Success\n",
      "Failed Agents: None\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read recent code executions for user 500 and display key columns\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        created_at, \n",
    "        user_id, \n",
    "        model_name, \n",
    "        is_successful, \n",
    "        initial_code,\n",
    "        latest_code,\n",
    "        failed_agents,\n",
    "        output\n",
    "    FROM code_executions \n",
    "    WHERE user_id = 500\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "df = run_query(query)\n",
    "\n",
    "# Print the results in a readable format\n",
    "for idx, row in df.iterrows():\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Date: {row['created_at']}\")\n",
    "    print(f\"User: {row['user_id']}\")\n",
    "    print(f\"Model: {row['model_name']}\")\n",
    "    print(f\"Initial Code: {row['initial_code']}\")\n",
    "    print(f\"Latest Code: {row['latest_code']}\")\n",
    "    print(f'Output: {row[\"output\"]}')\n",
    "    print(f\"Status: {'Success' if row['is_successful'] else 'Failed'}\")\n",
    "    print(f\"Failed Agents: {row['failed_agents']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
